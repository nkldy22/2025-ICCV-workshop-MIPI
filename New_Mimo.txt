nohup: ignoring input
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Path already exists. Rename it to /home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/experiments/mimo_perturbed_archived_20250710_032116
Path already exists. Rename it to /home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/tb_logger/mimo_perturbed_archived_20250710_032116
2025-07-10 03:21:16,467 INFO: 
                ____                _       _____  ____
               / __ ) ____ _ _____ (_)_____/ ___/ / __ \
              / __  |/ __ `// ___// // ___/\__ \ / /_/ /
             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/
            /_____/ \__,_//____//_/ \___//____//_/ |_|
     ______                   __   __                 __      __
    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /
   / / __ / __ \ / __ \ / __  /  / /   / / / // ___// //_/  / /
  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/
  \____/ \____/ \____/ \____/  /_____/\____/ \___//_/|_|  (_)
    
Version Information: 
	fewlens: 1.4.2
	PyTorch: 2.0.1+cu118
	TorchVision: 0.15.2+cu118
2025-07-10 03:21:16,468 INFO: 
  name: mimo_perturbed
  model_type: RestorationModel
  scale: 1
  num_gpu: 4
  manual_seed: 0
  datasets:[
    train:[
      name: General_Image_Train
      type: Dataset_from_h5
      dataroot_h5: /mnt/sda/zsh/dataset/AberrationCorrection-MIPI-dataset/train_dataset_pertubed.h5
      sigma: 5
      recrop_size: 256
      io_backend:[
        type: disk
      ]
      use_shuffle: True
      num_worker_per_gpu: 8
      batch_size_per_gpu: 8
      mini_batch_sizes: [8]
      iters: [200000]
      gt_size: 256
      gt_sizes: [256]
      dataset_enlarge_ratio: 1
      prefetch_mode: None
      phase: train
      scale: 1
    ]
  ]
  network_g:[
    type: MIMOUNet
    input_channel: 3
  ]
  path:[
    pretrain_network_g: None
    strict_load: True
    experiments_root: /home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/experiments/mimo_perturbed
    models: /home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/experiments/mimo_perturbed/models
    training_states: /home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/experiments/mimo_perturbed/training_states
    log: /home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/experiments/mimo_perturbed
    visualization: /home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/experiments/mimo_perturbed/visualization
  ]
  train:[
    optim_g:[
      type: Adam
      lr: 0.0001
      weight_decay: 0.0001
      betas: [0.9, 0.999]
    ]
    scheduler:[
      type: MultiStepLR
      milestones: [10000, 20000, 30000, 40000, 50000, 60000, 70000]
      gamma: 0.5
    ]
    total_iter: 200000
    warmup_iter: -1
    pixel_SRN_opt:[
      type: SRN_loss
      loss_weight: 1.0
      reduction: mean
    ]
    fft_opt:[
      type: Multi_FFT_loss
      loss_weight: 0.1
      reduction: mean
    ]
  ]
  val:[
    val_freq: 10000000.0
    save_img: True
    isp: False
    key_metric: psnr
    metrics:[
      psnr:[
        type: calculate_psnr
        crop_border: 4
        test_y_channel: False
      ]
      ssim:[
        type: calculate_ssim
        crop_border: 4
        test_y_channel: False
      ]
    ]
  ]
  logger:[
    print_freq: 100
    save_checkpoint_freq: 10000.0
    save_latest_freq: 100000.0
    show_tf_imgs_freq: 1000.0
    use_tb_logger: True
  ]
  dist_params:[
    backend: nccl
    port: 16500
  ]
  find_unused_parameters: True
  dist: True
  rank: 0
  world_size: 2
  auto_resume: False
  is_train: True
  root_path: /home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main

2025-07-10 03:21:16,883 INFO: Dataset [Dataset_from_h5] - General_Image_Train is built.
2025-07-10 03:21:16,883 INFO: Training statistics:
	Number of train images: 153600
	Dataset enlarge ratio: 1
	Batch size per gpu: 8
	World size (gpu number): 2
	Require iter number per epoch: 9600
	Total epochs: 21; iters: 200000.
2025-07-10 03:21:16,960 INFO: Network [MIMOUNet] is created.
2025-07-10 03:21:18,263 INFO: Network: DistributedDataParallel - MIMOUNet, with parameters: 6,592,995
2025-07-10 03:21:18,263 INFO: MIMOUNet(
  (Encoder): ModuleList(
    (0): EBlock(
      (layers): Sequential(
        (0): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (1): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (2): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (3): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (4): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (5): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (6): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (7): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
      )
    )
    (1): EBlock(
      (layers): Sequential(
        (0): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (1): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (2): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (3): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (4): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (5): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (6): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (7): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
      )
    )
    (2): EBlock(
      (layers): Sequential(
        (0): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (1): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (2): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (3): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (4): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (5): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (6): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
        (7): ResBlock(
          (main): Sequential(
            (0): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU(inplace=True)
              )
            )
            (1): BasicConv(
              (main): Sequential(
                (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              )
            )
          )
        )
      )
    )
  )
  (feat_extract): ModuleList(
    (0): BasicConv(
      (main): Sequential(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (1): BasicConv(
      (main): Sequential(
        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (2): BasicConv(
      (main): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (3): BasicConv(
      (main): Sequential(
        (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (4): BasicConv(
      (main): Sequential(
        (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (5): BasicConv(
      (main): Sequential(
        (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (Decoder): ModuleList(
    (0): TransformerDBlock(
      (layers): Sequential(
        (0): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)
            (project_in): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (project_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)
            (project_in): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (project_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (2): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)
            (project_in): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (project_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (3): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)
            (project_in): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (project_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (4): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)
            (project_in): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (project_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (5): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)
            (project_in): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (project_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (6): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)
            (project_in): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (project_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (7): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
            (project_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)
            (project_in): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
            (project_out): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (1): TransformerDBlock(
      (layers): Sequential(
        (0): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)
            (project_in): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
            (project_out): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)
            (project_in): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
            (project_out): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (2): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)
            (project_in): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
            (project_out): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (3): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)
            (project_in): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
            (project_out): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (4): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)
            (project_in): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
            (project_out): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (5): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)
            (project_in): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
            (project_out): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (6): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)
            (project_in): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
            (project_out): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (7): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (project_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4, bias=False)
            (project_in): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
            (project_out): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (2): TransformerDBlock(
      (layers): Sequential(
        (0): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
            (project_in): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (project_out): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
            (project_in): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (project_out): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (2): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
            (project_in): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (project_out): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (3): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
            (project_in): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (project_out): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (4): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
            (project_in): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (project_out): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (5): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
            (project_in): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (project_out): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (6): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
            (project_in): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (project_out): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (7): TransformerBlock(
          (norm1): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (attn): Attention(
            (qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (qkv_dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (project_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (relu): ReLU()
            (att_ca): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm2): LayerNorm(
            (body): WithBias_LayerNorm()
          )
          (ffn): FeedForward(
            (partial_conv3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)
            (project_in): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
            (project_out): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
  )
  (Convs): ModuleList(
    (0): BasicConv(
      (main): Sequential(
        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (1): BasicConv(
      (main): Sequential(
        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
  )
  (ConvsOut): ModuleList(
    (0): BasicConv(
      (main): Sequential(
        (0): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (1): BasicConv(
      (main): Sequential(
        (0): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (AFFs): ModuleList(
    (0): AFF(
      (conv): Sequential(
        (0): BasicConv(
          (main): Sequential(
            (0): Conv2d(224, 32, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU(inplace=True)
          )
        )
        (1): BasicConv(
          (main): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
    )
    (1): AFF(
      (conv): Sequential(
        (0): BasicConv(
          (main): Sequential(
            (0): Conv2d(224, 64, kernel_size=(1, 1), stride=(1, 1))
            (1): ReLU(inplace=True)
          )
        )
        (1): BasicConv(
          (main): Sequential(
            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
    )
  )
  (FAM1): FAM(
    (merge): BasicConv(
      (main): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (SCM1): SCM(
    (main): Sequential(
      (0): BasicConv(
        (main): Sequential(
          (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
        )
      )
      (1): BasicConv(
        (main): Sequential(
          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
          (1): ReLU(inplace=True)
        )
      )
      (2): BasicConv(
        (main): Sequential(
          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
        )
      )
      (3): BasicConv(
        (main): Sequential(
          (0): Conv2d(64, 125, kernel_size=(1, 1), stride=(1, 1))
          (1): ReLU(inplace=True)
        )
      )
    )
    (conv): BasicConv(
      (main): Sequential(
        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (FAM2): FAM(
    (merge): BasicConv(
      (main): Sequential(
        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
  (SCM2): SCM(
    (main): Sequential(
      (0): BasicConv(
        (main): Sequential(
          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
        )
      )
      (1): BasicConv(
        (main): Sequential(
          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))
          (1): ReLU(inplace=True)
        )
      )
      (2): BasicConv(
        (main): Sequential(
          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
        )
      )
      (3): BasicConv(
        (main): Sequential(
          (0): Conv2d(32, 61, kernel_size=(1, 1), stride=(1, 1))
          (1): ReLU(inplace=True)
        )
      )
    )
    (conv): BasicConv(
      (main): Sequential(
        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
)
2025-07-10 03:21:18,265 INFO: Loss [SRN_loss] is created.
2025-07-10 03:21:18,265 INFO: Loss [Multi_FFT_loss] is created.
2025-07-10 03:21:18,266 INFO: Model [RestorationModel] is created.
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-07-10 03:21:38,848 INFO: Start training from epoch: 0, iter: 0
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
2025-07-10 03:22:00,822 INFO: 
 Updating Patch_Size to 256 and Batch_Size to 16 

Traceback (most recent call last):
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/train.py", line 284, in <module>
    train_pipeline(root_path)
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/train.py", line 224, in train_pipeline
    model.optimize_parameters(current_iter)
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/models/restoration_model.py", line 204, in optimize_parameters
    preds = self.net_g(self.lq)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/archs/MIMO_arch.py", line 562, in forward
    z = self.Decoder[2](z)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/archs/MIMO_arch.py", line 434, in forward
    return self.layers(x)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/archs/MIMO_arch.py", line 414, in forward
    x = x + self.attn(self.norm1(x))
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/archs/MIMO_arch.py", line 298, in forward
    out = (attn @ v)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 23.70 GiB total capacity; 22.13 GiB already allocated; 5.12 MiB free; 22.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/train.py", line 284, in <module>
    train_pipeline(root_path)
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/train.py", line 224, in train_pipeline
    model.optimize_parameters(current_iter)
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/models/restoration_model.py", line 204, in optimize_parameters
    preds = self.net_g(self.lq)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/archs/MIMO_arch.py", line 562, in forward
    z = self.Decoder[2](z)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/archs/MIMO_arch.py", line 434, in forward
    return self.layers(x)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/archs/MIMO_arch.py", line 414, in forward
    x = x + self.attn(self.norm1(x))
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/zsh/code/Aberration_Correction_MIPI-main/fewlens/archs/MIMO_arch.py", line 302, in forward
    out = self.project_out(out)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 1; 23.70 GiB total capacity; 22.19 GiB already allocated; 16.88 MiB free; 22.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 30030) of binary: /home/ubuntu13/anaconda3/envs/fewlens/bin/python
Traceback (most recent call last):
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/distributed/run.py", line 798, in <module>
    main()
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu13/anaconda3/envs/fewlens/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
fewlens/train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-07-10_03:22:08
  host      : ubuntu
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 30031)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-07-10_03:22:08
  host      : ubuntu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 30030)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
